services:
    # Backend app (Django running under gunicorn). We proxy via nginx below.
    backend:
        build:
            context: ./BackEnd/iot_backend
            dockerfile: dockerfile
        container_name: poly-backend
        networks:
            default:
                aliases:
                    - web
        environment:
            DJANGO_ALLOWED_HOSTS: "localhost,127.0.0.1,nginx"
            DJANGO_DEBUG: "False"
            OLLAMA_BASE_URL: "http://ollama:11434"
        volumes:
            - staticfiles:/staticfiles
        depends_on:
            ollama:
                condition: service_healthy
        command: sh -c "python manage.py migrate && python manage.py collectstatic --noinput && gunicorn iot_backend.wsgi:application --bind 0.0.0.0:8000 --workers 3"

    # Nginx reverse proxy for Django + serves /static/
    nginx:
        image: nginx:alpine
        container_name: poly-nginx
        depends_on:
            - backend
        ports:
            - "8080:80"
        volumes:
            - staticfiles:/staticfiles:ro
            - ./BackEnd/iot_backend/nginx.conf:/etc/nginx/conf.d/default.conf:ro

    # Frontend (SvelteKit preview for now)
    frontend:
        build:
            context: ./Frontend
            dockerfile: dockerfile
            args:
                - VITE_BACKEND_URL=http://localhost:8080
        container_name: poly-frontend
        environment:
            - VITE_BACKEND_URL=http://localhost:8080
        ports:
            - "4173:4173"
        depends_on:
            - nginx

    # Ollama model service
    ollama:
        image: ollama/ollama:latest
        container_name: poly-ollama
        ports:
            - "11434:11434"
        volumes:
            - ollama:/root/.ollama
        healthcheck:
            test: ["CMD", "ollama", "list"]
            interval: 10s
            timeout: 5s
            retries: 30
            start_period: 10s
        restart: unless-stopped

    # Optional init to pull a model at startup
    ollama-init:
        image: ollama/ollama:latest
        container_name: poly-ollama-init
        environment:
            - OLLAMA_HOST=http://ollama:11434
        depends_on:
            ollama:
                condition: service_healthy
        volumes:
            - ollama:/root/.ollama
        entrypoint: ["/bin/sh", "-lc"]
        command: >
            "set -e;
             echo 'Pulling llama3 model...';
             ollama pull llama3;
             echo 'Done.'"
        restart: "no"

volumes:
    staticfiles:
    ollama:
